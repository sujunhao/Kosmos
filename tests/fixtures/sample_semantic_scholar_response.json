{
  "total": 150000,
  "offset": 0,
  "next": 10,
  "data": [
    {
      "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "corpusId": 13756489,
      "title": "Attention Is All You Need",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
      "venue": "NIPS",
      "year": 2017,
      "referenceCount": 44,
      "citationCount": 98765,
      "influentialCitationCount": 12345,
      "isOpenAccess": true,
      "fieldsOfStudy": ["Computer Science"],
      "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2017-06-12",
      "journal": {
        "name": "Advances in Neural Information Processing Systems",
        "pages": "5998-6008",
        "volume": "30"
      },
      "authors": [
        {
          "authorId": "1699545",
          "name": "Ashish Vaswani"
        },
        {
          "authorId": "2328694",
          "name": "Noam Shazeer"
        },
        {
          "authorId": "2255826",
          "name": "Niki Parmar"
        },
        {
          "authorId": "1753291",
          "name": "Jakob Uszkoreit"
        },
        {
          "authorId": "32659715",
          "name": "Llion Jones"
        },
        {
          "authorId": "2118765",
          "name": "Aidan N. Gomez"
        },
        {
          "authorId": "3078844",
          "name": "Lukasz Kaiser"
        },
        {
          "authorId": "1757488",
          "name": "Illia Polosukhin"
        }
      ],
      "externalIds": {
        "ArXiv": "1706.03762",
        "DBLP": "conf/nips/VaswaniSPUJGKP17",
        "MAG": "2963403421",
        "DOI": "10.5555/3295222.3295349"
      },
      "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "citations": [
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "year": 2019
        },
        {
          "paperId": "a8e0d685f0d3e6e0e6e1a4e5d0e8e7e9e0e1e2e3",
          "title": "GPT-3: Language Models are Few-Shot Learners",
          "year": 2020
        }
      ],
      "references": [
        {
          "paperId": "1f89427d6e5e4b5b5f5e4e5d0e8e7e9e0e1e2e3",
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "year": 2015
        }
      ]
    },
    {
      "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
      "corpusId": 52967399,
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
      "venue": "NAACL",
      "year": 2019,
      "referenceCount": 76,
      "citationCount": 87654,
      "influentialCitationCount": 10234,
      "isOpenAccess": true,
      "fieldsOfStudy": ["Computer Science"],
      "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2018-10-11",
      "journal": {
        "name": "Proceedings of the 2019 Conference of the North",
        "pages": "4171-4186"
      },
      "authors": [
        {
          "authorId": "1786546",
          "name": "Jacob Devlin"
        },
        {
          "authorId": "2252901",
          "name": "Ming-Wei Chang"
        },
        {
          "authorId": "2076953",
          "name": "Kenton Lee"
        },
        {
          "authorId": "1699545",
          "name": "Kristina Toutanova"
        }
      ],
      "externalIds": {
        "ArXiv": "1810.04805",
        "DBLP": "conf/naacl/DevlinCLT19",
        "MAG": "2951055622",
        "DOI": "10.18653/v1/N19-1423"
      },
      "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
      "citations": [],
      "references": [
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention Is All You Need",
          "year": 2017
        }
      ]
    },
    {
      "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
      "corpusId": 206592766,
      "title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art results.",
      "venue": "NIPS",
      "year": 2012,
      "referenceCount": 38,
      "citationCount": 125000,
      "influentialCitationCount": 15000,
      "isOpenAccess": false,
      "fieldsOfStudy": ["Computer Science"],
      "publicationTypes": ["JournalArticle", "Conference"],
      "publicationDate": "2012-12-03",
      "journal": {
        "name": "Communications of the ACM",
        "pages": "84-90",
        "volume": "60"
      },
      "authors": [
        {
          "authorId": "1753244",
          "name": "Alex Krizhevsky"
        },
        {
          "authorId": "1699545",
          "name": "Ilya Sutskever"
        },
        {
          "authorId": "1737648",
          "name": "Geoffrey E. Hinton"
        }
      ],
      "externalIds": {
        "MAG": "2138284181",
        "DBLP": "conf/nips/KrizhevskySH12",
        "DOI": "10.1145/3065386"
      },
      "url": "https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff",
      "citations": [],
      "references": []
    }
  ]
}
